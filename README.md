Run A MCP server locally with node js, ollama and any function supported ollama models

you would need
- node
- ollama
- RAM (24GB, 16GB might also work)

You build
- Local api server to host simple apis
- Local MCP server to talk to the api
- Local MCP client integrated with LLM and connect to MCP
